\documentclass[pdf,preprint]{iucr}
\RequirePackage{graphicx}
\journalcode{J}

\usepackage[utf8]{inputenc}

\usepackage{amsmath}
\usepackage[separate-uncertainty=true]{siunitx}
\usepackage[version=4]{mhchem}


\begin{document}
\title{refnx -- Neutron and X-ray reflectometry analysis in Python}
\cauthor[a]{Andrew~R. J.}{Nelson}{andrew.nelson@ansto.gov.au}{}
\author[b]{Stuart~W.}{Prescott}

\aff[a]{Australian Nuclear Science and Technology Organisation, Locked Bag 2001, Kirrawee DC, NSW 2232 \country{Australia}}
\aff[b]{School of Chemical Engineering, University of New South Wales,  Sydney, NSW, 2052 \country{Australia}}

\date{\today}

\newcommand{\refnx}{\emph{refnx}}
\newcommand{\Objective}{\textbf{Objective}}
\newcommand{\CurveFitter}{\textbf{CurveFitter}}
\newcommand{\GlobalObjective}{\textbf{GlobalObjective}}
\newcommand{\Parameter}{\textbf{Parameter}}
\newcommand{\Structure}{\textbf{Structure}}
\newcommand{\Slab}{\textbf{Slab}}
\newcommand{\Component}{\textbf{Component}}
\newcommand{\LipidLeaflet}{\textbf{LipidLeaflet}}
\newcommand{\Transform}{\textbf{Transform}}
\newcommand{\DataD}{\textbf{Data1D}}
\newcommand{\ReflectModel}{\textbf{ReflectModel}}
\newcommand{\CurveFitter}{\textbf{CurveFitter}}
\newcommand{\Spline}{\textbf{Spline}}
\newcommand{\conda}{\emph{conda}}
\newcommand{\corner}{\emph{corner}}
\newcommand{\MixedReflectModel}{\textbf{MixedReflectModel}}
\newcommand{\pip}{\emph{pip}}
\newcommand{\emcee}{\emph{emcee}}
\newcommand{\ptemcee}{\emph{ptemcee}}
\newcommand{\NumPy}{\emph{NumPy}}
\newcommand{\SciPy}{\emph{SciPy}}
\newcommand{\Cython}{\emph{Cython}}
\newcommand{\Jupyter}{\emph{Jupyter}}
\newcommand{\ipywidgets}{\emph{ipywidgets}}

\maketitle

\begin{synopsis}
%TODO synopsis for ToC
\end{synopsis}

%TODO en_GB or en_US?

\begin{abstract}
\refnx\ is a model-based neutron and X-ray reflectometry data analysis package written in Python. It is cross platform, and has been tested on Linux, macOS, and Windows. Its graphical user interface is browser-based, through a \Jupyter\ notebook.
Model construction is modular, being composed from a series of components that each describe a subset of the interface, parameterised in terms of physically relevant parameters (volume fraction of a polymer, lipid area per molecule, etc). The model and data are used to create an objective, which is used to calculate residuals, log-likelihood, and log-prior probabilities of the system. Objectives are combined to perform co-refinement of multiple datasets, and mixed-area models. Prior knowledge of parameter values are encoded as probability distribution functions or bounds on all parameters in the system. Extra prior probability terms can be defined for Components, over and above those available from the Parameters alone. Algebraic Parameter constraints are available.
A choice of fitting approaches is available, including least-squares (global and gradient-based optimizers) and a Bayesian approach using Markov Chain Monte Carlo to investigate the posterior distribution of the model parameters. The Bayesian approach is useful in examining parameter covariances, model selection, and variability in the resulting scattering length density profiles.
The package is designed to facilitate reproducible research; its use in \Jupyter\ notebooks, and subsequent distribution of those notebooks as supporting information, permits straightforward reproduction of analyses.
\end{abstract}

\section{Introduction}

The use of specular X-ray and neutron reflectometry for morphological characterisation of thin films on the approximate size range 10 to \SI{5000}{\angstrom} has grown remarkably over the past years \cite{Wood2017, Daillant2009}. Most neutron and X-ray sources have instruments to perform reflectometry measurements, and there is an ongoing need for accessible software programs for users of those instruments to analyse their data in a straightforward fashion, including the co-refinement of multiple contrast datasets.  Several programs are available for this purpose, with a variety of different features \cite{Nelson2006,Bjorck2007,Kienzle2011,Gerelli2016}. These programs typically create a model of the interface, and incrementally refine the model against the data using least-squares, or use Bayesian approaches \cite{Sivia2006, 2010arXiv1008.4686H} to examine the posterior probability distribution of the parameters (i.e.\ the statistical variation of the parameters in a model).

Given the number of publications arising from the reflectometry technique, it is vital that analyses are reproducible as well as the experiments that are undertaken. Reproducibility in research is an underlying principle of science; unfortunately, it is not always possible to reproduce the results of others \cite{Stark2018}, because there is frequently not enough information provided in journal articles to repeat the analyses. Even if the datasets and software packages used to analyse them are supplied in supporting information (most often they are not), a comprehensive, ordered, set of instructions or a codified workflow would need to be provided \cite{Moeller2017a}.
One example for addressing this reproducibility issue are the guidelines in the small-angle scattering community for the deposition of data and and associated models \cite{Trewhella:jc5010}.

Here, we outline a new reflectometry analysis package, \refnx\ (version number 0.1 is used in this paper \cite{refnx}), that helps address the reproducibility issue for the reflectometry community\footnote{We do not mean that other programs are irreproducible, rather that the information provided by scientists in journal articles is often lacking.} by creating a scripted analysis workflow that is readily published alongside the publication, such as we have done with this paper (see the Supporting Information). The \refnx\ Python package is specifically designed for use in \Jupyter\ notebooks \cite{Kluyver:2016aa}, which provide a literate programming environment that mixes executable code cells, rich documentation of the steps that were performed, and the computational output. By including the analysis as performed by the authors in such a notebook and appending it as supporting information along with the data, readers are empowered to replicate the exact data analysis and potentially extend the analysis, provided they have set up the same computing environment. Setting up the computing environment is simplified using the \conda\ package manager \cite{conda}, and an environment file (although other approaches are available).

%TODO cite https://osf.io/h9gsd/ "Millman and Pï¿½rez, Developing open source scientific practice" ?

% FIXME next para can be deleted? (dupe)
\refnx\ has a wide range of sophisticated modelling capabilities including: parameter constraints, description of prior probabilities on parameters using arbitrary probability distribution functions, modular construction of models using a variety of components that are user extensible, co-refinement of datasets, handling of patchy layers, a graphical user interface, resolution handling, Markov Chain Monte Carlo sampling of the posterior probability distributions for the parameters, model selection, etc.

% FIXME this next section doesn't fit in very well right here
The posterior probability distribution for a parameter set in Bayesian statistics, Equation~\ref{eqn:1}, is proportional to the product of the prior probability and the likelihood (or the sum of the log-probabilities):
%
\begin{gather} 
\label{eqn:1}\ p(\theta | D, I) \propto p(\theta | I)\times p(D | \theta, I)\\
p(D | \theta, I) = -\frac{1}{2} \sum \left[\left(\frac{y_n - model} {\sigma_n}\right)^2 + log(\sigma_n^2)\right]\label{eqn:2}
\end{gather}
%
The prior distribution, $p(\theta | I)$, is the probability distribution function for a parameter, $\theta$, given preexisting knowledge of the system, $I$. For example, a prior could be a simple uniform distribution that specifies a lower and upper bound, or a normal distribution that represents an experimentally derived value and associated uncertainty.
The likelihood (Equation~\ref{eqn:2}), $p(D | \theta, I)$, is the probability of the observed data, $D$, given the model parameters and other prior information. It is calculated from the measured data, $y_n$ (with uncertainties $\sigma_n$), and the generative model.

The posterior distribution, $p(\theta | I, D)$, is the probability distribution function of the parameters given the data and prior knowledge about the system.  The posterior probability is sampled by encoding the likelihood and prior distributions and using Markov Chain Monte Carlo (MCMC) techniques \cite{emcee, ptemcee}. At the end of an MCMC run the parameter set possesses a number of samples (called a `chain'), whose statistics reveal the distribution and covariance of the parameters, the spread of the model around the data, and in a reflectometry context the range of structures that are consistent with the data.

\section{Method}

\refnx\ is written in Python with an object-oriented design, Figure~\ref{fig:components}. As with \emph{Motofit} \cite{Nelson2006} it calculates reflectivity using the Abeles method \cite{Heavens1955} for specular reflection from a stratified medium.
The building block of the analysis is the \Parameter\ object which represents a model value, whether it is allowed to vary in a fit, and a bounds attribute. The bounds are a probability distribution representing the prior. Any of the \emph{scipy.stats} \cite{Jones2001-2017} continuous distributions, or other distributions created by the user, can be used for this purpose. Algebraic relationships between \Parameter\ objects can be applied using constraints.

The \Structure\ object represents the interfacial model, comprising of individual \Component\ objects in series. Each \Component\ represents a subset of the interface and contains several physically relevant Parameters. The simplest \Component\ is a \Slab\, which has a uniform scattering length density (SLD), thickness, roughness, and volume fraction of solvent. The simplest models are simply a series of \Slab objects. More sophisticated components include \LipidLeaflet\ and \Spline. \Spline\ is designed for freeform modelling of an SLD profile using spline interpolation. In addition to its constituent parameters, each \Component\ can also contribute to the log-prior probability. This is useful when a \Component\ has a derived value, such as surface excess, which is already known, and can be included as prior knowledge of the system. Each \Component\ has a \emph{slabs} property that represents one or more (micro)slices for its particular subset of the interface. A \Slab\ object has a single slice because it is a single thickness of uniform SLD. A \LipidLeaflet\ can be made of two slices (head/tail regions), but the \Spline\ has many thin slices approximating the smooth curve. Each of these slices has uniform SLD, the Nevot-Croce approach used to describe the roughness between them \cite{Nevot1980}; the user can create different roughness models by creating a `microsliced' \Component.

The \Structure\ object is used to construct a \ReflectModel\ object. This object is responsible for calculating the resolution smeared reflectivity of the \Structure, scaling the data, and adding a $Q$-independent linear background (via the scale and background \Parameter\ objects). There are different types of smearing available: constant $\mathrm{d}Q/Q$, point-by-point resolution smearing read from the dataset of interest, or via a smearing probability kernel of arbitrary shape \cite{Nelson2014}. The constant $\mathrm{d}Q/Q$ and point-by-point smearing use Gaussian convolution, with $\mathrm{d}Q$ representing the full width half maximum (FWHM) of a Gaussian approximation to the instrument resolution function \cite{Well2005}.

The \Objective\ class uses the \ReflectModel\ and a dataset, \DataD, to calculate $\chi^2$, log-likelihood, log-prior, residuals, and the generative model.
The \DataD\ object has \emph{x, y, y\_err, x\_err} attributes to represent $Q$, $R$, $\mathrm{d}R$, $\mathrm{d}Q$. The \DataD\ object can be constructed from a tuple of arrays representing the data, or by reading a three or four column plain-text datafile. A three column dataset represents $Q$ (\si{\per\angstrom}), $R$, $\mathrm{d}R$ (1 standard deviation). A four column dataset represents $Q$, $R$, $\mathrm{d}R$, $\mathrm{d}Q$ (\si{\per\angstrom}). Subclassing \DataD\ would allow other formats to be read. One example of this could be an wavelength dispersive file using $\Omega$, $\lambda$ data instead of $Q$; such as that used in energy scanned X-ray reflectometry, or sometimes produced by wavelength dispersive neutron reflectometers. In such a case \ReflectModel\ could be subclassed to make full use of this energy dispersive information.
An \Objective\ can be given a \Transform\ object to permit fitting as $\log_{10} R$ vs $Q$, $RQ^4$ vs $Q$; the default (no \Transform) is $R$ vs $Q$. Several \Objective\ can be combined to form a \GlobalObjective\ for co-refinement. The object-oriented nature allows \Parameter\, \Component\ to be reused in in different \Objective, thereby facilitating co-refinement.

The \Objective\ statistics are used directly by the \CurveFitter\ class to perform least-square fitting or Bayesian MCMC of the system. \refnx\ uses the \SciPy\ package for least-squares analysis (DifferentialEvolution, Levenberg--Marquardt, LBFGSB - Limited Broyden--Fletcher--Goldfarb--Shanno with bounds) and the \emcee\ and \ptemcee\ packages \cite{emcee, ptemcee} for affine invariant MCMC ensemble sampling of the posterior distribution of the parameters. \ptemcee\ is a fork of \emcee\ that implements parallel tempering, for characterisation of multi-modal probability distributions; different modes can be traversed by chain populations at higher `temperatures', while individual modes are efficiently explored by chains at lower `temperatures'.
The likelihoods used in MCMC and least-squares assume that the measurement errors are normally distributed, Equation~\ref{eqn:2}. However, other types of measurement errors (e.g. Poissonian) could be implemented by a subclass of \Objective\ overriding the log-likelihood method.
Parallelisation of the sampling automatic, and can use MPI on a cluster for greater parallelisation. Visualisation of the samples produced by MCMC sampling is conveniently performed using the \corner\ package \cite{corner} for scatter plot matrices. On the conclusion of a fit/sampling run each \Parameter\ value is updated and given a standard error. For the sampling, these represent the median and half the $[15.87, 84.13]$ percentile range respectively; the latter approximating the standard deviation for a normally distributed statistic.

\begin{figure}
  \includegraphics[width=85mm]{components}
  \caption{Schematic showing the relationship between classes that make up a typical reflectometry curve-fitting problem.}
  \label{fig:components}
\end{figure}

To make it easy to create quick models, and to ease the learning curve, an ipywidgets \cite{ipywidgets} based fitting graphical user interface (GUI) can be instantiated within a \Jupyter\ notebook, Figure~\ref{fig:gui}. This GUI builds slab based models for analysing a single dataset, but can be used as a stepping point for building more advanced models via the `To code' code generation button. A fully functional web-based reflectometry analysis notebook is currently available \cite{Nelson2018}. For a comprehensive demonstration of multiple contrast co-refinement, see the annotated notebook in the supporting information.

\begin{figure}
  \includegraphics[width=85mm]{./supporting_information/gui.png}
  \caption{Screenshot of the \Jupyter/\ipywidgets\ GUI; this \Jupyter\ notebook is available in the supporting information.}
  \label{fig:gui}
\end{figure}

\subsection{Example data analysis}

Neutron reflectometry is an ideal technique for the study of biologically relevant lipid membrane mimics and their interactions with proteins, etc. Multiple contrast variation measurements are necessary to reduce modelling ambiguity (due to loss of phase information in the scattering experiment) and improve the ability to determine the structure of various components in the system. The gold standard approach for analysis of these datasets is co-refinement with a common model, and to parameterise the model in terms of chemically relevant parameters, such as the area per molecule. Sometimes a patchy coverage (distinct to low area per molecule) necessitates the use of an (incoherent) sum of reflectivities from different areas. \refnx\ has functionality for all these requirements, such as the \LipidLeaflet\ component for describing the head and tail groups of a lipid leaflet, and \MixedReflectModel to add reflectivities. It is straightforward to develop/modify new components for different structural functionality, as a consequence of the object-oriented approach.

The parameters used in the \LipidLeaflet\ component are: area per molecule ($A$), thicknesses for each of the head and tail regions ($t_x$), sums of scattering lengths of the head and tail regions ($b_x$), partial volumes of the head and tail groups ($V_x$), roughness between head and tail region, and SLDs of the solvents for the head and tail group ($\rho_{x,\mathrm{solv}}$). The overall SLD of each of the head and tail group regions are given by:
%TODO cite a paper which gives a whole load of volumes?
\begin{gather} 
\label{eqn:3} \phi_{x} = \frac{V_x}{At_x}\\
\rho_x =  \phi_{x} \frac{b_x}{V_x} + (1 - \phi_{x})\rho_{x,\mathrm{solv}} \label{eqn:4}
\end{gather}
The approach used in \LipidLeaflet\ component ensures that there is a 1:1 correspondence of heads to tails.
By default the head and tail solvents are assumed to be the same as the solvent that is used throughout the \Structure. This will be the case when using \LipidLeaflet\ for a solid-liquid reflectometry experiment. However, at the air-liquid, or liquid-liquid interfaces the solvent for the head and tail region may be different, and it is possible to use different solvent SLDs for each. We note that the \LipidLeaflet\ component may also be used to describe other amphiphiles adsorbing at an interface.

Here, \LipidLeaflet\ is used to co-refine three contrasts (\ce{D_2O}, \ce{Si} contrast match [hdmix, SLD=\SI{2.07E-6}{\per\square\angstrom}], and \ce{H_2O}) of a 1,2-dimyristoyl-sn-glycero-3-phospho\-choline (DMPC) bilayer at the solid-liquid interface, Figure~\ref{fig:global_fit}.\footnote{The validity of \LipidLeaflet\ does depend on the area per molecule being equal for the headgroup and tailgroup regions, as pointed out by Gerelli \cite{Gerelli2016}, which can be violated if there are guest molecules that insert in the membrane.} Two \LipidLeaflet\ objects are required to describe the inner and outer leaflets of a bilayer, hence, the component contains an attribute which can reverse the direction of one of the leaflets. The use of individual objects to describe each leaflet leads to great flexibility; it becomes easy to model asymmetric bilayers (inner leaflet can be a different lipid to the outer lipid), and one can model interstitial water layers between the leaflets as well.

The \Jupyter\ notebook used for the analysis, \emph{lipid.ipynb}, is available in the supporting information. The corner plot (Figure~\ref{fig:corner}) produced from the MCMC analysis shows the covariance between parameters, with an area per molecule of \SI{57.0 \pm0.15}{\square\angstrom}. Figure~\ref{fig:global_fit} shows the probability distribution of the generative model around the data and in the SLD profile. These are obtained by random samples from the MCMC chain. The spread in SLD profiles is used to determine what range of structures is consistent with the data. Multi-modalities in these SLD profiles can be due to statistical uncertainties, the $Q$ ranges measured, and the loss of phase information in NR \cite{Majkrzak1999, Heinrich2009}.
%TODO A reference for V_m?
%FIXME units of area per molecule were wrong; check

\begin{figure}%
\centering
\label{fig:global_fit}%
\includegraphics[width=85mm]{./supporting_information/global_fit.png}%
%TODO: series labels on plots; delete points?;
%TODO: artefact in high q d2o fit; truncate at Q = 0/35 ?
%TODO: h2o fit isn't great about 0.20
%TODO: yaxis labels even, ylim?

\includegraphics[width=85mm]{./supporting_information/d2o_sld_spread.png}

%TODO add (a) and (b)
\caption{a) Neutron reflectivity from a DMPC bilayer supported on a silicon crystalmeasured at three contrasts, with 500 samples from the posterior distribution in grey and median of the distribution in red. Data for the hdmix and \ce{H_2O} contrast offset by 0.1 and 0.001 respectively. b) SLD profile of the \ce{D_2O} model showing 500 samples from the posterior distribution, as well as the median in red. It is seen that the uncertainty in the reflectivity at high $Q$ is associated with an uncertainty in SLD profile at the lipid-\ce{D2O} interface.}
\end{figure}

\begin{figure}
  \includegraphics[width=175mm]{./supporting_information/corner.png}
  \caption{Corner plot for the varying parameters of DMPC bilayers measured at three contrasts. The sampling took $\sim$13 minutes on a \SI{3.2}{GHz} quad-core computer for 30 saved steps, corresponding to 6000 samples, with the steps being thinned by a factor of 100.}
  \label{fig:corner}
\end{figure}

 
\section{Distribution and Modification}

Each submodule in \refnx\ possesses its own unit testing code for checking that the functions and classes in the module operate correctly, both individually and collectively. For example, there are tests that check that the reflectivity of a model is calculated correctly, or that the behaviour of a function is correct for the different possible inputs and code paths through it. Since the test suite is an integral part of the package each installation is testable. In addition, there is a benchmarking suite to track changes in performance, specifically the speed of critical calculations, over time.
 
The source code for \refnx\ is held in a version controlled git repository hosted on GitHub.\footnote{https://www.github.com/refnx/refnx} The standard GitHub workflow is followed in which contributors create their own `fork' of the main \refnx\ repository, and create a feature branch to which they make modifications. They then submit a pull request (PR) against the main repository. The modifications made in the PR are checked on continuous integration (CI) web-services that run the test suite against a matrix of Python versions on the macOS, Linux and Windows operating systems. Features are merged into the main repository if all tests pass, and if manual code review concludes that the changes are scientifically correct, of sufficiently high standard, and useful. When a sufficient number of features have accumulated, a new release is made. Successive releases have an incrementing semantic version number which can be obtained from the installed package, with each release being given its own DOI.

The recommended way of using \refnx\ is from a \conda\ environment, which offers package, dependency and environment management \cite{conda}, using the pre-compiled distributables on the \refnx\ conda-forge channel. These distributables are made as part of the release process using the same CI web-services used to test the code. The matrix of distributables covers the major Python versions currently in use, across the macOS, Windows, and Linux operating systems. Alternatively the package can be installed from source, either directly from the git repository, or via \pip\ from the version uploaded to PyPI.\footnote{https://pypi.python.org/pypi/refnx} Building from source requires a C compiler and the \Cython\ and \NumPy\ packages to be installed; further dependencies should be installed to run the test suite to verify that compilation and installation was successful.

% unless refnx has an absolute promise of permanent backwards compatibility, conda packages of the current version will not run old analysis code and so conda won't be usable for the reproducibility aspect.

% with wheels for refnx, is conda any easier than 'pip install refnx'

% FIXME: is only numpy required for building? more is required to run the test suite and people should do so?

\refnx\ is released under the BSD permissive open source licence. In addition its dependencies are released under open source licences which means that use is free of cost to the end user and also the user is free to modify and improve this software.

%FIXME refnx doesn't actually have a BSD licence; it has a BSD-like licence

\section{Comments on reproducibility of analyses}

In order for a given scattering analysis to be fully reproducible by others, a general set of conditions need to be met:
\begin{itemize}
  \item the datasets used need to be deposited with a journal article, or be freely available.
  %FIXME is that the raw data or the reduced data?
  \item the exact software environment needs to be recreatable.
  %FIXME refnx does not address this further down the stack
  \item the exact ordered set of steps taken during the analysis needs to be listed.
\end{itemize} 
Each of these points are often omitted in the literature. For example, the use of different software versions may change the output of an analysis, or the use of a GUI program may preclude recording the full set of steps, or options, applied by a user. Whilst it is unable to meet the first criterion by itself, the use of \refnx\ in a \Jupyter\ notebook can fulfil the other two requirements, providing a little care is taken. As we have already notes, the ordered set of steps to perform the analysis is the \Jupyter\ notebook in which the analysis was performed and this is an artefact able to be archived.

The exact software environment can be recreated by noting down the versions of the software packages used during an analysis (\refnx, \SciPy, \NumPy, Python, etc). At a later date those exact versions can be installed in the same Python version using one of: the \conda\ package manager, by installing from the source at a given version tag in the git repository, or by \pip. \conda\ can use environment files to recreate a specific setup. An alternative way of recreating the environment is by using a virtual machine, or other container environment such as Docker; the strengths and weaknesses of various software distribution practices and the relationship with reproducible science has been discussed in detail elsewhere \cite{Moeller2017a}.

The usefulness of open-source software in a git (or other version controlled) repository must be emphasised here. With closed source or proprietary software, the ability to return to a specific software version/environment can be frustrated, and different versions can have modifications that can unknowingly change the output of an analysis. In addition accessibility (due to cost, etc) to the wider scientific community can also hinder reproducibility.
% careful here: older Python and numpy releases won't even compile any more with newer compilers and ancillary libraries so you have to do the entire stack not 'just get a tag'

Moreover, there are important ramifications for verifiability. \refnx\ is based on a fully open software stack, with good unit test coverage. The user can run tests for each component and inspect parts for correctness. For example, the behaviour of the reflectivity calculation in \refnx\ is checked from first principles in the test suite; and can be done now and in several years time. If problems are discovered, they can be corrected. With a closed source program such checking is harder, one has to examine the inputs/outputs of a black box without knowing what is happening in between, making it harder to find errors; one has to rely on trust. Whilst this may not be an issue for relatively simple calculations, it may be for complex algorithms, such as those involved with minimisation.
%include citation

\section{Conclusions}\label{conclusions}

\refnx\ is a powerful tool for least-squares or Bayesian analysis of neutron and X-ray reflectometry data that is ideally usable for reproducible research with \Jupyter\ notebooks, and has been built with extensibility in mind. Its features include: MCMC sampling of posterior distribution for parameters, structural models constructed from modular components with physically relevant parameterisation, algebraic inter-parameter constraints, mixed area models, co-refinement of multiple datasets, probability distributions for parameter bounds used directly for log-prior terms, and a (\Jupyter\ ) \ipywidgets\ GUI.

\ack{Acknowledgements:
We acknowledge Anton Le Brun (ANSTO) for the provision of the lipid bilayer datasets in the example, James Hester (ANSTO) for comments made on the draft manuscript, and Andrew McCluskey (Bath University) and Isaac Gresham (UNSW) for important feedback on \refnx\ development.}

% FIXME: Does the data have a DOI? is the raw data in the supp info? reduced data?

\section{Supporting information}

\noindent
\textbf{gui.ipynb} - \Jupyter\ notebook used to create the GUI screenshot.\\
\textbf{lipid.ipynb} - \Jupyter\ notebook used for the lipid analysis example.\\
\textbf{lipid.pdf} - PDF view of the \Jupyter\ notebook used for the lipid analysis example.\\
\textbf{refnx-paper.yml} - \conda\ environment file to reproduce the analysis environment in this paper.

\bibliography{main}{}
\bibliographystyle{iucr}
\end{document}
